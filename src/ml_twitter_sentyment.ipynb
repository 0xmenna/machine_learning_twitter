{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Twitter Sentiment Classification Project**\n",
    "\n",
    "- [1. Introduction](#1-introduction)\n",
    "  - [Recurrent Neural Networks](#recurrent-neural-networks)\n",
    "  - [High Level Steps](#high-level-steps)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "In this project, we tackle the **binary classification** of **Twitter messages** into **positive** and **negative** categories. We are provided with two separate directories containing labeled tweets:\n",
    "\n",
    "- Directory **0**: Negative tweets\n",
    "- Directory **1**: Positive tweets\n",
    "\n",
    "Our approach focuses on a detailed exploration of a single model family rather than broadly comparing multiple families, which could lead to overly generic solutions. By conducting a series of experiments with an emphasis on originality, we aim to dig deeper into the specific characteristics of the chosen model family. This approach allows us to experiment with architecture, hyperparameters, and training strategies in a more targeted way, ultimately evaluating the accuracy of the models on the validation data properly extracted from the dataset.\n",
    "\n",
    "### **Recurrent Neural Networks**\n",
    "\n",
    "We chose to focus on experimenting with Recurrent Neural Networks (RNNs) for the sentiment analysis task. RNNs are particularly well-suited for processing sequential data, as they maintain a hidden state that captures the influence of previous input sequences. Tweets, being short and concise, often require understanding the flow of sentiment within a limited context. RNNs excel at this by processing one word at a time, retaining contextual information from previous words to identify patterns that signal positive or negative sentiment.\n",
    "\n",
    "Although Transformers are the state-of-the-art for many NLP tasks, we specifically opted not to use them here. Transformers are optimized for modeling long-range dependencies and contextual relationships across entire documents. While highly effective, their self-attention mechanism can be unnecessary for short texts like tweets. Additionally, Transformers are computationally intensive, demanding significant resources for training and inference. For a simpler binary classification task, this added complexity can result in inefficiencies without a proportional gain in performance.\n",
    "\n",
    "By choosing RNNs, we focus on a model that is both resource-efficient and well-matched to the specific demands of analyzing short texts, ensuring effective results without unnecessary computational overhead.\n",
    "\n",
    "### **High Level Steps**\n",
    "\n",
    "From a high level, the project will follow these steps:\n",
    "\n",
    "1. Load and preprocess the text data.\n",
    "2. Experiment with **different model configurations**.\n",
    "3. Evaluate and compare these configurations using validation data and standard metrics.\n",
    "4. Test the final model on a proper testing set.\n",
    "5. Check how the model behaves on **personal tweets**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Dependencies and some utility functions for the notebook\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Removes info and warning messages from Tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def create_rich_table(data, headers, title=\"Table\"):\n",
    "    table = Table(title=title, show_lines=True)\n",
    "\n",
    "    # Add headers to the table\n",
    "    for header in headers:\n",
    "        table.add_column(header, justify=\"center\")\n",
    "\n",
    "    # Add rows to the table\n",
    "    for row in data:\n",
    "        table.add_row(*map(str, row))\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def print_best_hyperparameters(best_hpm, model_name):\n",
    "    best_hp_table_data = [(key, best_hp.get(key)) for key in best_hp.values]\n",
    "    best_hyper_params_table = create_rich_table(best_hp_table_data, headers=[\"Hyperparameter\", \"Value\"], title=\"Best Hyperparameters for \" + model_name)\n",
    "\n",
    "    console.print(best_hyper_params_table)\n",
    "\n",
    "\n",
    "def evaluate_model_and_print_results(model, model_name, test_ds):\n",
    "    # Evaluate on the test set\n",
    "    evaluation_results = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    # Prepare the results as a table\n",
    "    metric_names = model.metrics_names\n",
    "    eval_table_data = [(metric, result) for metric, result in zip(metric_names, evaluation_results)]\n",
    "\n",
    "    # Print the results\n",
    "    evaluation_table = create_rich_table(eval_table_data, headers=[\"Metric\", \"Result\"], title=\"Evaluation Results for \" + model_name)\n",
    "\n",
    "    console.print(evaluation_table)\n",
    "\n",
    "# Plot metrics from training history\n",
    "def plot_training_history(history):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for metric in history_df.columns:\n",
    "        plt.plot(history_df[metric], label=metric)\n",
    "\n",
    "    plt.title(\"Training Metrics over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Dataset Loading and Preprocessing**\n",
    "\n",
    "In this section, we focus on loading the dataset, preprocessing the text, and applying vectorization. Our goal is to maintain a modular approach throughout the project to ensure reusability and adaptability of functions, particularly for experimenting with different hyperparameter configurations (e.g., `max_tokens`, `output_sequence_length`).\n",
    "\n",
    "**Steps Involved**\n",
    "\n",
    "1. Dataset Loading\n",
    "2. Creating a `TextVectorization` layer, adaptable to various text preprocessing configurations\n",
    "\n",
    "The dataset is loaded through the following configurations:\n",
    "\n",
    "- Batch Size: 32. We opted for a memory-efficient approach by using smaller batch sizes. Since RNNs process data sequentially, larger batches would have significantly increased memory usage. Additionally, given the not too large size of the dataset, smaller batch sizes enable more frequent gradient updates per epoch.\n",
    "\n",
    "- Splits: 70% for training, 20% for validation and 10% for testing\n",
    "\n",
    "For the `TextVectorization` layer we used a custom standardization function, tailored for twitter messages, specific for sentyment analysis. In particular, the following rules are applied:\n",
    "\n",
    "1. Convert to Lowercase\n",
    "2. Remove HTTP URLs\n",
    "3. Remove Hashtags\n",
    "4. Keep only alphanumeric, spaces, and specific punctuation (!, ?, ...), useful for sentyment analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149985 files belonging to 2 classes.\n",
      "Using 104990 files for training.\n",
      "Found 149985 files belonging to 2 classes.\n",
      "Using 44995 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Dataset Summary                      </span>\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Dataset   </span>┃<span style=\"font-weight: bold\"> Number of Tweets </span>┃<span style=\"font-weight: bold\"> Negative % </span>┃<span style=\"font-weight: bold\"> Positive % </span>┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  Training  │      104990      │   50.08%   │   49.92%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│ Validation │      30016       │   49.85%   │   50.15%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│  Testing   │      14979       │   49.85%   │   50.15%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│   Total    │      149985      │   50.01%   │   49.99%   │\n",
       "└────────────┴──────────────────┴────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Dataset Summary                      \u001b[0m\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Dataset  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNumber of Tweets\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNegative %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPositive %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  Training  │      104990      │   50.08%   │   49.92%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│ Validation │      30016       │   49.85%   │   50.15%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│  Testing   │      14979       │   49.85%   │   50.15%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│   Total    │      149985      │   50.01%   │   49.99%   │\n",
       "└────────────┴──────────────────┴────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_DIR = \"../TwitterParsed\"\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    data_dir, batch_size=32, validation_split=0.2, test_split=0.1, seed=42\n",
    "):\n",
    "    # Training dataset\n",
    "    train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split + test_split,  # Total non-training data\n",
    "        subset=\"training\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Split with both validation and test sets\n",
    "    val_and_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split + test_split,\n",
    "        subset=\"validation\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Further split into validation and test sets\n",
    "    val_size = math.floor(\n",
    "        (validation_split / (validation_split + test_split)) * len(val_and_test_ds)\n",
    "    )\n",
    "\n",
    "    val_ds = val_and_test_ds.take(val_size)\n",
    "    test_ds = val_and_test_ds.skip(val_size)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def create_vectorization_layer(max_tokens=10000, sequence_length=100):\n",
    "    # Custom standardization function tailored for tweets\n",
    "    def custom_standardize(input_text):\n",
    "        # Lowercase the text\n",
    "        lowercase_text = tf.strings.lower(input_text)\n",
    "        # Remove URLs\n",
    "        text_without_urls = tf.strings.regex_replace(lowercase_text, r\"http\\S+\", \" \")\n",
    "        # Remove mentions (e.g., @username)\n",
    "        text_without_mentions = tf.strings.regex_replace(\n",
    "            text_without_urls, r\"@\\w+\", \" \"\n",
    "        )\n",
    "        # Replace hashtags with just the word (e.g., #happy -> happy)\n",
    "        text_without_hashtags = tf.strings.regex_replace(\n",
    "            text_without_mentions, r\"#\", \"\"\n",
    "        )\n",
    "        # Replace two or more dots with a placeholder\n",
    "        text_with_dots_preserved = tf.strings.regex_replace(\n",
    "            text_without_hashtags, r\"\\.{2,}\", \"<MULTI_DOT>\"\n",
    "        )\n",
    "        # Remove all single periods\n",
    "        text_without_single_dots = tf.strings.regex_replace(\n",
    "            text_with_dots_preserved, r\"\\.\", \"\"\n",
    "        )\n",
    "        # Restore the multi-dot sequences\n",
    "        text_with_restored_dots = tf.strings.regex_replace(\n",
    "            text_without_single_dots, r\"<MULTI_DOT>\", \"...\"\n",
    "        )\n",
    "        # Keep only alphanumeric, spaces, and specific punctuation (!, ?, ...)\n",
    "        cleaned_text = tf.strings.regex_replace(\n",
    "            text_with_restored_dots, r\"[^a-z0-9\\s!?...]\", \"\"\n",
    "        )\n",
    "        return cleaned_text\n",
    "\n",
    "    # Create the TextVectorization layer\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,  # Vocabulary size\n",
    "        output_mode=\"int\",  # Map tokens to integers\n",
    "        output_sequence_length=sequence_length,  # Pad/Truncate to sequence length\n",
    "        standardize=custom_standardize,  # Use the custom standardization logic\n",
    "    )\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "# Count dataset samples\n",
    "def count_samples(dataset):\n",
    "    return sum(1 for _ in dataset.unbatch())\n",
    "\n",
    "\n",
    "def compute_class_distribution(dataset):\n",
    "    neg_count = 0\n",
    "    pos_count = 0\n",
    "    for _, label in dataset.unbatch():\n",
    "        if label.numpy() == 0:\n",
    "            neg_count += 1\n",
    "        else:\n",
    "            pos_count += 1\n",
    "    total = neg_count + pos_count\n",
    "    neg_percent = (neg_count / total) * 100 if total > 0 else 0\n",
    "    pos_percent = (pos_count / total) * 100 if total > 0 else 0\n",
    "    return neg_count, pos_count, neg_percent, pos_percent\n",
    "\n",
    "\n",
    "def create_dataset_summary_table(train_ds, val_ds, test_ds):\n",
    "    train_count = count_samples(train_ds)\n",
    "    val_count = count_samples(val_ds)\n",
    "    test_count = count_samples(test_ds)\n",
    "    total_count = train_count + val_count + test_count\n",
    "\n",
    "    train_neg, train_pos, train_neg_percent, train_pos_percent = (\n",
    "        compute_class_distribution(train_ds)\n",
    "    )\n",
    "    val_neg, val_pos, val_neg_percent, val_pos_percent = compute_class_distribution(\n",
    "        val_ds\n",
    "    )\n",
    "    test_neg, test_pos, test_neg_percent, test_pos_percent = compute_class_distribution(\n",
    "        test_ds\n",
    "    )\n",
    "\n",
    "    # Prepare the data for the table\n",
    "    data = [\n",
    "        [\n",
    "            \"Training\",\n",
    "            train_count,\n",
    "            f\"{train_neg_percent:.2f}%\",\n",
    "            f\"{train_pos_percent:.2f}%\",\n",
    "        ],\n",
    "        [\"Validation\", val_count, f\"{val_neg_percent:.2f}%\", f\"{val_pos_percent:.2f}%\"],\n",
    "        [\"Testing\", test_count, f\"{test_neg_percent:.2f}%\", f\"{test_pos_percent:.2f}%\"],\n",
    "        [\n",
    "            \"Total\",\n",
    "            total_count,\n",
    "            f\"{((train_neg + val_neg + test_neg) / total_count) * 100:.2f}%\",\n",
    "            f\"{((train_pos + val_pos + test_pos) / total_count) * 100:.2f}%\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Headers for the table\n",
    "    headers = [\"Dataset\", \"Number of Tweets\", \"Negative %\", \"Positive %\"]\n",
    "\n",
    "    # Creeate a rich table\n",
    "    rich_table = create_rich_table(data, headers, title=\"Dataset Summary\")\n",
    "\n",
    "    return rich_table\n",
    "\n",
    "\n",
    "# Load dataset in training (70%), validation (20%), and test (10%) sets\n",
    "train_ds, val_ds, test_ds = load_dataset(DATASET_DIR)\n",
    "summary_table = create_dataset_summary_table(train_ds, val_ds, test_ds)\n",
    "console.print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Baseline RNN**\n",
    "\n",
    "WWe’ll start by implementing a basic Recurrent Neural Network (RNN) as a baseline, skipping LSTMs for now. This will let us observe how a simpler model performs without the computational and memory overhead of LSTMs, which use memory cells to handle long-term dependencies. Since our sequences are long, the RNN is expected to struggle with retaining information from earlier steps. By comparing this baseline to an LSTM later, we aim to evaluate how much LSTMs can improve performance and at what cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |64                |embedding_dim\n",
      "16                |16                |rnn_units\n",
      "orthogonal        |orthogonal        |rnn_recurrent_initializer\n",
      "96                |96                |dense_units\n",
      "glorot_uniform    |glorot_uniform    |kernel_initializer\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m 277/3281\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:01\u001b[0m 1s/step - accuracy: 0.4992 - loss: 0.6965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Start the search for the best hyperparameters\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEBUGGING_LOG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Train the model with the best hyperparameters\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/dev/machine_learning_twitter/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "MODEL_NAME = \"simple_rnn\"\n",
    "TUNING_DIR = \"../Tuning\"\n",
    "\n",
    "VOCAB_SIZES = [5000, 10000]\n",
    "MAX_SEQ_LENGTHS = [50, 75]\n",
    "\n",
    "EMBEDDING_DIMS = [32, 64]\n",
    "\n",
    "DEBUGGING_LOG = 1\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# The only purpose of this is to set the seeds and have same results over different runs\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# Build model for Keras Tuner\n",
    "def build_model(hp, use_dropout=False, use_layernorm=False):\n",
    "    \"\"\"\n",
    "    This function defines a simple RNN-based model without memory cells,\n",
    "    and includes hyperparameters for tuning via Keras Tuner.\n",
    "    \"\"\"\n",
    "    # 1. Add the vectorization layer with the choice of max_tokens and output_sequence_length\n",
    "    hp_max_tokens = hp.Choice(\"max_tokens\", values=VOCAB_SIZES)\n",
    "    hp_out_seq_len = hp.Choice(\"output_sequence_length\", values=MAX_SEQ_LENGTHS)\n",
    "\n",
    "    vectorizer = create_vectorization_layer(\n",
    "        max_tokens=hp_max_tokens, sequence_length=hp_out_seq_len\n",
    "    )\n",
    "    train_text = train_ds.map(lambda x, y: x)  # remove labels\n",
    "    vectorizer.adapt(train_text)\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(vectorizer)\n",
    "\n",
    "    # 2. Add the embedding layer, with the choice of embedding dimension\n",
    "    embedding_dim = hp.Choice(\"embedding_dim\", EMBEDDING_DIMS)\n",
    "    model.add(layers.Embedding(input_dim=hp_max_tokens, output_dim=embedding_dim))\n",
    "\n",
    "    # 3. Add a SimpleRNN layer with the choice of number of units, recurrent_initializer, and recurrent_dropout\n",
    "    model.add(\n",
    "        layers.SimpleRNN(\n",
    "            units=hp.Int(\"rnn_units\", min_value=16, max_value=128, step=16),\n",
    "            activation=\"tanh\",\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            recurrent_initializer=hp.Choice(\n",
    "                \"rnn_recurrent_initializer\", [\"orthogonal\", \"glorot_uniform\"]\n",
    "            ),\n",
    "            recurrent_dropout=hp.Float(\n",
    "                \"rnn_recurrent_dropout\",\n",
    "                min_value=0.0,\n",
    "                max_value=0.3,\n",
    "                step=0.1,\n",
    "            ),\n",
    "            return_sequences=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 4. Optional: Add a LayerNormalization layer\n",
    "    if use_layernorm:\n",
    "        model.add(layers.LayerNormalization())\n",
    "\n",
    "    # 5. Add a Dense layer with the choice of number of units and kernel_initializer\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=hp.Int(\n",
    "                \"dense_units\", min_value=64, max_value=256, step=32, default=128\n",
    "            ),\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=hp.Choice(\n",
    "                \"kernel_initializer\", [\"he_normal\", \"glorot_uniform\"]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 6. Optional: Add Dropout layer\n",
    "    if use_dropout:\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "\n",
    "    # 7: Final output layer for binary classification\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # 8. Compile the model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Tuner configuration for optimal hyperparameters\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    "    directory=TUNING_DIR,\n",
    "    project_name=\"simple_rnn_tuning\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Start the search for the best hyperparameters\n",
    "tuner.search(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    verbose=DEBUGGING_LOG,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)],\n",
    ")\n",
    "\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "history = best_model.fit(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    verbose=DEBUGGING_LOG,\n",
    "    epochs=50,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)],\n",
    ")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print_best_hyperparameters(best_hp, MODEL_NAME)\n",
    "\n",
    "# Evaluate the best model on the test set and print evaluation results\n",
    "eval_results = evaluate_model_and_print_results(best_model, MODEL_NAME, test_ds)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
