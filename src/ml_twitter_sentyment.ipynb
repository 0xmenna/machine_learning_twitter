{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Twitter Sentiment Classification Project**\n",
    "\n",
    "- [1. Introduction](#1-introduction)\n",
    "  - [Recurrent Neural Networks](#recurrent-neural-networks)\n",
    "  - [High Level Steps](#high-level-steps)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "In this project, we tackle the **binary classification** of **Twitter messages** into **positive** and **negative** categories. We are provided with two separate directories containing labeled tweets:\n",
    "\n",
    "- Directory **0**: Negative tweets\n",
    "- Directory **1**: Positive tweets\n",
    "\n",
    "Our approach focuses on a detailed exploration of a single model family rather than broadly comparing multiple families, which could lead to overly generic solutions. By conducting a series of experiments with an emphasis on originality, we aim to dig deeper into the specific characteristics of the chosen model family. This approach allows us to experiment with architecture, hyperparameters, and training strategies in a more targeted way, ultimately evaluating the accuracy of the models on the validation data properly extracted from the dataset.\n",
    "\n",
    "### **Recurrent Neural Networks**\n",
    "\n",
    "We chose to focus on experimenting with Recurrent Neural Networks (RNNs) for the sentiment analysis task. RNNs are particularly well-suited for processing sequential data, as they maintain a hidden state that captures the influence of previous input sequences. Tweets, being short and concise, often require understanding the flow of sentiment within a limited context. RNNs excel at this by processing one word at a time, retaining contextual information from previous words to identify patterns that signal positive or negative sentiment.\n",
    "\n",
    "Although Transformers are the state-of-the-art for many NLP tasks, we specifically opted not to use them here. Transformers are optimized for modeling long-range dependencies and contextual relationships across entire documents. While highly effective, their self-attention mechanism can be unnecessary for short texts like tweets. Additionally, Transformers are computationally intensive, demanding significant resources for training and inference. For a simpler binary classification task, this added complexity can result in inefficiencies without a proportional gain in performance.\n",
    "\n",
    "By choosing RNNs, we focus on a model that is both resource-efficient and well-matched to the specific demands of analyzing short texts, ensuring effective results without unnecessary computational overhead.\n",
    "\n",
    "### **High Level Steps**\n",
    "\n",
    "From a high level, the project will follow these steps:\n",
    "\n",
    "1. Load and preprocess the text data.\n",
    "2. Experiment with **different model configurations**.\n",
    "3. Evaluate and compare these configurations using validation data and standard metrics.\n",
    "4. Test the final model(s) on **personal tweets** to see how they generalize.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Removes info and warning messages from Tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def create_rich_table(data, headers, title=\"Table\"):\n",
    "    table = Table(title=title, show_lines=True)\n",
    "\n",
    "    # Add headers to the table\n",
    "    for header in headers:\n",
    "        table.add_column(header, justify=\"center\")\n",
    "\n",
    "    # Add rows to the table\n",
    "    for row in data:\n",
    "        table.add_row(*map(str, row))\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Dataset Loading and Preprocessing**\n",
    "\n",
    "In this section, we focus on loading the dataset, preprocessing the text, and applying vectorization. Our goal is to maintain a modular approach throughout the project to ensure reusability and adaptability of functions, particularly for experimenting with different hyperparameter configurations (e.g., `max_tokens`, `output_sequence_length`).\n",
    "\n",
    "**Steps Involved**\n",
    "\n",
    "1. Dataset Loading\n",
    "2. Creating a `TextVectorization` layer, adaptable to various text preprocessing configurations\n",
    "\n",
    "The dataset is loaded through the following configurations:\n",
    "\n",
    "- Batch Size: 32. We opted for a memory-efficient approach by using smaller batch sizes. Since RNNs process data sequentially, larger batches would have significantly increased memory usage. Additionally, given the not too large size of the dataset, smaller batch sizes enable more frequent gradient updates per epoch.\n",
    "\n",
    "- Splits: 70% for training, 20% for validation and 10% for testing\n",
    "\n",
    "For the `TextVectorization` layer we used a custom standardization function, tailored for twitter messages, specific for sentyment analysis. In particular, the following rules are applied:\n",
    "\n",
    "1. Convert to Lowercase\n",
    "2. Remove HTTP URLs\n",
    "3. Remove Hashtags\n",
    "4. Removed hashtags\n",
    "5. Keep only alphanumeric, spaces, and specific punctuation (!, ?, ...), useful for sentyment analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149985 files belonging to 2 classes.\n",
      "Using 104990 files for training.\n",
      "Found 149985 files belonging to 2 classes.\n",
      "Using 44995 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Dataset Summary                      </span>\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Dataset   </span>┃<span style=\"font-weight: bold\"> Number of Tweets </span>┃<span style=\"font-weight: bold\"> Negative % </span>┃<span style=\"font-weight: bold\"> Positive % </span>┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  Training  │      104990      │   50.08%   │   49.92%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│ Validation │      30016       │   49.92%   │   50.08%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│  Testing   │      14979       │   49.90%   │   50.10%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│   Total    │      149985      │   50.03%   │   49.97%   │\n",
       "└────────────┴──────────────────┴────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Dataset Summary                      \u001b[0m\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Dataset  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNumber of Tweets\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNegative %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPositive %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  Training  │      104990      │   50.08%   │   49.92%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│ Validation │      30016       │   49.92%   │   50.08%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│  Testing   │      14979       │   49.90%   │   50.10%   │\n",
       "├────────────┼──────────────────┼────────────┼────────────┤\n",
       "│   Total    │      149985      │   50.03%   │   49.97%   │\n",
       "└────────────┴──────────────────┴────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_DIR = \"../TwitterParsed\"\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    data_dir, batch_size=32, validation_split=0.2, test_split=0.1, seed=42\n",
    "):\n",
    "\n",
    "    # Training dataset\n",
    "    train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split + test_split,  # Total non-training data\n",
    "        subset=\"training\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Split containing validation and test sets\n",
    "    val_and_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split + test_split,\n",
    "        subset=\"validation\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Further split validation into validation and test datasets\n",
    "    val_size = math.floor(\n",
    "        (validation_split / (validation_split + test_split)) * len(val_and_test_ds)\n",
    "    )\n",
    "\n",
    "    val_ds = val_and_test_ds.take(val_size)\n",
    "    test_ds = val_and_test_ds.skip(val_size)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def create_vectorization_layer(max_tokens=10000, sequence_length=100):\n",
    "\n",
    "    # Custom standardization function tailored for tweets\n",
    "    def custom_standardize(input_text: tf.Tensor) -> tf.Tensor:\n",
    "        # Lowercase the text\n",
    "        lowercase_text = tf.strings.lower(input_text)\n",
    "        # Remove URLs\n",
    "        text_without_urls = tf.strings.regex_replace(lowercase_text, r\"http\\S+\", \" \")\n",
    "        # Remove mentions (e.g., @username)\n",
    "        text_without_mentions = tf.strings.regex_replace(\n",
    "            text_without_urls, r\"@\\w+\", \" \"\n",
    "        )\n",
    "        # Replace hashtags with just the word (e.g., #happy -> happy)\n",
    "        text_without_hashtags = tf.strings.regex_replace(\n",
    "            text_without_mentions, r\"#\", \"\"\n",
    "        )\n",
    "        # Remove single periods (.) but retain the multiple ones\n",
    "        text_without_single_dots = tf.strings.regex_replace(\n",
    "            text_without_hashtags, r\"(?<!\\.)\\.(?!\\.)\", \"\"\n",
    "        )\n",
    "        # Collapse multiple dots (.....) into a single ellipsis (...) for consistency\n",
    "        text_with_ellipses = tf.strings.regex_replace(\n",
    "            text_without_single_dots, r\"\\.{2,}\", \"...\"\n",
    "        )\n",
    "        # Keep only alphanumeric, spaces, and specific punctuation (!, ?, ...)\n",
    "        cleaned_text = tf.strings.regex_replace(\n",
    "            text_with_ellipses, r\"[^a-z0-9\\s!?...]\", \"\"\n",
    "        )\n",
    "        return cleaned_text\n",
    "\n",
    "    # Create the TextVectorization layer\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,  # Vocabulary size\n",
    "        output_mode=\"int\",  # Map tokens to integers\n",
    "        output_sequence_length=sequence_length,  # Pad/Truncate to sequence length\n",
    "        standardize=custom_standardize,  # Use the custom standardization logic\n",
    "    )\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "# Count dataset samples\n",
    "def count_samples(dataset):\n",
    "    return sum(1 for _ in dataset.unbatch())\n",
    "\n",
    "\n",
    "def compute_class_distribution(dataset):\n",
    "    neg_count = 0\n",
    "    pos_count = 0\n",
    "    for _, label in dataset.unbatch():\n",
    "        if label.numpy() == 0:\n",
    "            neg_count += 1\n",
    "        else:\n",
    "            pos_count += 1\n",
    "    total = neg_count + pos_count\n",
    "    neg_percent = (neg_count / total) * 100 if total > 0 else 0\n",
    "    pos_percent = (pos_count / total) * 100 if total > 0 else 0\n",
    "    return neg_count, pos_count, neg_percent, pos_percent\n",
    "\n",
    "\n",
    "def create_dataset_summary_table(train_ds, val_ds, test_ds):\n",
    "    # Calculate counts and class distributions\n",
    "    train_count = count_samples(train_ds)\n",
    "    val_count = count_samples(val_ds)\n",
    "    test_count = count_samples(test_ds)\n",
    "    total_count = train_count + val_count + test_count\n",
    "\n",
    "    train_neg, train_pos, train_neg_percent, train_pos_percent = (\n",
    "        compute_class_distribution(train_ds)\n",
    "    )\n",
    "    val_neg, val_pos, val_neg_percent, val_pos_percent = compute_class_distribution(\n",
    "        val_ds\n",
    "    )\n",
    "    test_neg, test_pos, test_neg_percent, test_pos_percent = compute_class_distribution(\n",
    "        test_ds\n",
    "    )\n",
    "\n",
    "    # Prepare the data for the table\n",
    "    data = [\n",
    "        [\n",
    "            \"Training\",\n",
    "            train_count,\n",
    "            f\"{train_neg_percent:.2f}%\",\n",
    "            f\"{train_pos_percent:.2f}%\",\n",
    "        ],\n",
    "        [\"Validation\", val_count, f\"{val_neg_percent:.2f}%\", f\"{val_pos_percent:.2f}%\"],\n",
    "        [\"Testing\", test_count, f\"{test_neg_percent:.2f}%\", f\"{test_pos_percent:.2f}%\"],\n",
    "        [\n",
    "            \"Total\",\n",
    "            total_count,\n",
    "            f\"{((train_neg + val_neg + test_neg) / total_count) * 100:.2f}%\",\n",
    "            f\"{((train_pos + val_pos + test_pos) / total_count) * 100:.2f}%\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Headers for the table\n",
    "    headers = [\"Dataset\", \"Number of Tweets\", \"Negative %\", \"Positive %\"]\n",
    "\n",
    "    # Creeate a rich table\n",
    "    rich_table = create_rich_table(data, headers, title=\"Dataset Summary\")\n",
    "\n",
    "    return rich_table\n",
    "\n",
    "\n",
    "# Load dataset in training (70%), validation (20%), and test (10%) sets\n",
    "train_ds, val_ds, test_ds = load_dataset(DATASET_DIR)\n",
    "summary_table = create_dataset_summary_table(train_ds, val_ds, test_ds)\n",
    "console.print(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
